# Training Parameters
experiment_name: TrainBlueAgentARTCampaign # The experiment name
seed: 1 # The seed to use for the experiment
deterministic: false # Whether or not the training is deterministic
device: cpu # Which device to train on
async_env: false # Whether to run the environments in parallel
total_timesteps: 1000000 # Total number of steps to run throughout training
num_saves: 10 # Number of times the model is evaluated and saved during the training run
num_envs: 4 # Number of game environments to run at once (parallel if async_env == true)
num_steps: 100 # Number of steps to run per episode
eval_episodes: 10 # Number of episodes to run in an evaluation

# Environment Parameters
environment: CyberwheelRL # Environment class to use
network_config: 15-host-network.yaml # Network configuration filename in config/network
host_config: host_defs_services.yaml # Host configuration filename in config/host_definitions
decoy_config: decoy_hosts.yaml # Decoy configuration filename in config/detector
red_agent: brute_force_encryption_campaign.yaml # Red agent configuration filename in config/red_agent
train_red: false
campaign: true
valid_targets: all
blue_agent: rl_blue_agent.yaml # Blue agent configuration filename in config/blue_agent
train_blue: true
reward_function: RLReward # The reward function
detector_config: detector_handler.yaml # Detector configuration filename in config/detector

# RL Parameters
env_id: cyberwheel # Environment ID
learning_rate: 2.5e-4 # Learning rate value of the optimizer
anneal_lr: true # Whether or not to anneal the learning rate for policy and value networks
gamma: 0.99 # The discount factor (gamma value)
gae_lambda: 0.95 # Lambda for the general advantage estimation
num_minibatches: 4 # Number of minibatches
update_epochs: 4 # Number of epochs to update the policy
norm_adv: true # Whether or not to normalize advantages
clip_coef: 0.2 # Surrogate clipping coefficient
clip_vloss: true # Whether to use clipped loss for the value function
ent_coef: 0.01 # Coefficient of entropy
vf_coef: 0.5 # Coefficient of the value function
max_grad_norm: 0.5 # The maximum norm for the gradient clipping
target_kl: null # The target KL Divergence Threshold

# Weights & Biases Parameters (optional)
track: false
wandb_project_name: # The W&B project name if tracking
wandb_entity: # The W&B entity if tracking