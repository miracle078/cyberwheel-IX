# Training Parameters
experiment_name: test_headstart_15hostnetwork_5decoys_withpostplay #Test_SULI_Headstart_Delay_5decoys_100steps_nopostplay # The experiment name
seed: 1 # The seed to use for the experiment
deterministic: false # Whether or not the training is deterministic
device: cpu # Which device to train on
async_env: true # Whether to run the environments in parallel
total_timesteps: 100000000 # Total number of steps to run throughout training; changed
num_saves: 10 # Number of times the model is evaluated and saved during the training run
num_envs: 50 # Number of game environments to run at once (parallel if async_env == true)
num_steps: 100 # Number of steps to run per episode
eval_episodes: 10 # Number of episodes to run in an evaluation

# Asymmetric
headstart: 5
after_headstart_blue_active: true
decoy_limit: 5
objective: delay # delay, downtime, detect, general

# Environment Parameters
environment: CyberwheelHS # Environment class to use
network_config: 15-host-network.yaml # Network configuration filename in config/network
host_config: host_defs_services.yaml # Host configuration filename in config/host_definitions
decoy_config: decoy_hosts.yaml # Decoy configuration filename in config/detector
red_agent: art_agent.yaml # Red agent configuration filename in config/red_agent
train_red: false
campaign: false
valid_targets: all
blue_agent: rl_blue_agent.yaml # Blue agent configuration filename in config/blue_agent
train_blue: true
reward_function: RLRewardAsymmetric # The reward function
detector_config: detector_handler.yaml # Detector configuration filename in config/detector

# RL Parameters
env_id: cyberwheel # Environment ID
learning_rate: 2.5e-4 # Learning rate value of the optimizer
anneal_lr: true # Whether or not to anneal the learning rate for policy and value networks
gamma: 0.99 # The discount factor (gamma value)
gae_lambda: 0.95 # Lambda for the general advantage estimation
num_minibatches: 4 # Number of minibatches
update_epochs: 4 # Number of epochs to update the policy
norm_adv: true # Whether or not to normalize advantages
clip_coef: 0.2 # Surrogate clipping coefficient
clip_vloss: true # Whether to use clipped loss for the value function
ent_coef: 0.01 # Coefficient of entropy
vf_coef: 0.5 # Coefficient of the value function
max_grad_norm: 0.5 # The maximum norm for the gradient clipping
target_kl: null # The target KL Divergence Threshold

# Weights & Biases Parameters (optional)
track: true
wandb_project_name: Cyberwheel # The W&B project name if tracking
wandb_entity: oeschsec # The W&B entity if tracking
