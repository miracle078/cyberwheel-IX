\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{enumitem}

% Custom environments for foundational explanations
\newtcolorbox{foundation}{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=Foundation Concept
}

\newtcolorbox{intuition}{
    colback=green!5!white,
    colframe=green!75!black,
    title=Intuitive Explanation
}

\newtcolorbox{mathdetails}{
    colback=red!5!white,
    colframe=red!75!black,
    title=Mathematical Details
}

\newtcolorbox{example}{
    colback=yellow!5!white,
    colframe=yellow!75!black,
    title=Concrete Example
}

\title{Cyberwheel Research: Foundational Step-by-Step Explanation}
\author{Complete Beginner's Guide to Understanding the Research}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{What Are We Actually Trying to Do? (The Big Picture)}

\begin{foundation}
\textbf{The Core Problem}: How can we train computer programs (AI agents) to automatically defend computer networks against cyber attacks, where both the attackers and defenders are learning and adapting to each other?

Think of this like teaching two chess players simultaneously - one trying to attack and win, the other trying to defend and prevent the attack - except instead of chess, it's cybersecurity.
\end{foundation}

\begin{intuition}
\textbf{Real-World Analogy}: Imagine you're training two security guards:
\begin{itemize}
\item \textbf{Red Team (Attacker)}: Tries to break into a building using various methods
\item \textbf{Blue Team (Defender)}: Tries to detect and stop the break-ins using cameras, alarms, and decoy rooms
\end{itemize}

Both teams get better over time by learning from their successes and failures. The defender learns where to place cameras and decoy rooms to catch attackers, while the attacker learns to avoid detection and find new ways in.
\end{intuition}

\section{What is Reinforcement Learning? (Starting from Zero)}

\begin{foundation}
\textbf{Reinforcement Learning (RL)} is a way to train computer programs by having them:
\begin{enumerate}
\item Try different actions in an environment
\item Get rewards (positive) or penalties (negative) based on their actions
\item Learn which actions lead to better rewards over time
\end{enumerate}

This is exactly how humans and animals learn - through trial and error with feedback.
\end{foundation}

\begin{example}
\textbf{Simple Example}: Teaching a computer to play Pac-Man
\begin{itemize}
\item \textbf{Environment}: The Pac-Man game maze
\item \textbf{Actions}: Move up, down, left, right
\item \textbf{Rewards}: +10 for eating a dot, +50 for eating a ghost, -100 for getting caught
\item \textbf{Learning}: Over many games, the computer learns strategies that maximize its total score
\end{itemize}
\end{example}

\subsection{Key RL Concepts We Need to Understand}

\begin{mathdetails}
\textbf{The Mathematical Framework}:
\begin{itemize}
\item \textbf{State (S)}: What the agent can observe about the environment
\item \textbf{Action (A)}: What the agent can do
\item \textbf{Reward (R)}: Feedback the agent receives
\item \textbf{Policy ($\pi$)}: The agent's strategy (which action to take in each state)
\item \textbf{Value Function (V)}: How good it is to be in a particular state
\end{itemize}

\textbf{The Goal}: Find a policy $\pi$ that maximizes the expected total reward:
$$J(\pi) = \mathbb{E}\left[\sum_{t=1}^{T} \gamma^{t-1} R_t\right]$$

Where:
\begin{itemize}
\item $T$ = total time steps
\item $\gamma$ = discount factor (0.95 in our research) - values future rewards less than immediate ones
\item $R_t$ = reward at time $t$
\end{itemize}
\end{mathdetails}

\section{What Makes This "Adversarial"?}

\begin{foundation}
\textbf{Adversarial Learning} means we have two (or more) agents learning simultaneously, where one agent's success often means the other's failure. This is different from single-agent RL where there's only one learner.
\end{foundation}

\begin{intuition}
\textbf{Think of it like}: Two players learning to play chess against each other
\begin{itemize}
\item Player 1 gets better at attacking
\item Player 2 gets better at defending  
\item As Player 1 improves, Player 2 must adapt to the new strategies
\item As Player 2 improves, Player 1 must find new ways to attack
\item This creates an "arms race" of improvement
\end{itemize}
\end{intuition}

\subsection{Why is Adversarial Learning Hard?}

\begin{mathdetails}
\textbf{The Mathematical Challenge}:

In single-agent RL, we optimize:
$$\max_{\pi} J(\pi)$$

In adversarial RL, we have a two-player game:
$$\max_{\pi^{(b)}} \min_{\pi^{(r)}} J^{(b)}(\pi^{(b)}, \pi^{(r)})$$

Where:
\begin{itemize}
\item $\pi^{(b)}$ = blue (defender) policy
\item $\pi^{(r)}$ = red (attacker) policy  
\item Blue wants to maximize their reward
\item Red wants to minimize blue's reward (maximize their own)
\end{itemize}

This is much harder because:
\begin{itemize}
\item The environment is no longer stationary (it changes as the opponent learns)
\item Training can become unstable if one agent learns much faster than the other
\item Finding equilibrium solutions is computationally challenging
\end{itemize}
\end{mathdetails}

\section{The Cybersecurity Environment (Step by Step)}

\subsection{What is the "Environment"?}

\begin{foundation}
\textbf{The Environment} is a simulated computer network with:
\begin{itemize}
\item Multiple computers (hosts) - from 15 to 10,000 in our experiments
\item Network connections between computers
\item Some computers have vulnerabilities (security weaknesses)
\item Some computers can be "decoys" (fake computers designed to trap attackers)
\end{itemize}
\end{foundation}

\begin{example}
\textbf{Concrete Network Example}:
\begin{itemize}
\item 15 computers in a small office network
\item 3 of them are servers (valuable targets)
\item 2 of them are decoy computers (look real but are traps)
\item 10 of them are regular workstations
\item Computers are connected in subnets (like floors in a building)
\end{itemize}
\end{example}

\subsection{What Can the Red Agent (Attacker) Do?}

\begin{intuition}
\textbf{Red Agent Actions} mirror real-world cyber attacks:
\begin{enumerate}
\item \textbf{Discovery}: Scan the network to find computers and services
\item \textbf{Reconnaissance}: Probe computers to find vulnerabilities
\item \textbf{Privilege Escalation}: Exploit vulnerabilities to gain access
\item \textbf{Impact}: Steal data or disrupt services on compromised computers
\end{enumerate}
\end{intuition}

\begin{mathdetails}
\textbf{Red Agent State Space}: $S^{(r)} \in \mathbb{R}^{d_r}$ where $d_r = 2|H| + |S| + 299$

This means the red agent observes:
\begin{itemize}
\item Current position (which computer they've compromised)
\item Knowledge of network topology (what they've discovered)
\item Current attack phase (discovery, reconnaissance, etc.)
\item Available attack techniques (295 from MITRE ATT\&CK framework)
\end{itemize}

\textbf{Action Space}: $|\mathcal{A}^{(r)}| = 12 \times |H|$ actions
\begin{itemize}
\item For each host $H$, there are 12 possible attack actions
\item Total actions scale with network size
\end{itemize}
\end{mathdetails}

\subsection{What Can the Blue Agent (Defender) Do?}

\begin{intuition}
\textbf{Blue Agent Actions} mirror real-world cyber defense:
\begin{enumerate}
\item \textbf{Deploy Decoys}: Place fake computers to mislead attackers
\item \textbf{Remove Decoys}: Take down decoys that aren't working
\item \textbf{Isolate Hosts}: Disconnect compromised computers from the network
\item \textbf{Do Nothing}: Sometimes the best action is to wait and observe
\end{enumerate}
\end{intuition}

\begin{mathdetails}
\textbf{Blue Agent State Space}: $S^{(b)} \in \mathbb{R}^{d_b}$ where $d_b = 3|H| + 2$

The blue agent observes:
\begin{itemize}
\item Current alerts (immediate warnings about suspicious activity)
\item Alert history (memory of past attacks)
\item Decoy deployments (where fake computers are placed)
\item Metadata (constant values and counts)
\end{itemize}

\textbf{Action Space}: $|\mathcal{A}^{(b)}| = 2|S||\mathcal{D}| + |H| + 1$
\begin{itemize}
\item Deploy or remove decoys on subnets $S$ with decoy types $\mathcal{D}$
\item Isolate any of the $|H|$ hosts
\item Plus one "do nothing" action
\end{itemize}
\end{mathdetails}

\section{The Reward System (How Agents Learn What's Good/Bad)}

\begin{foundation}
\textbf{Rewards} tell the agents whether their actions were good or bad. This is how they learn over time.
\end{foundation}

\subsection{Red Agent Rewards}

\begin{intuition}
Red agent gets rewards for:
\begin{itemize}
\item \textbf{Successful attacks} on real computers (+points)
\item \textbf{Advancing through attack phases} (+points)
\item \textbf{Getting detected} (-points) - penalty for being caught
\end{itemize}
\end{intuition}

\begin{mathdetails}
\textbf{Red Reward Formula}:
$$R^{(r)}_{t,h} = \sum_i \alpha_i \cdot \mathbf{1}[\text{technique}_i \text{ successful}] + \beta \cdot |\text{assets compromised}| - \lambda \cdot \mathbf{1}[\text{detected}]$$

Where:
\begin{itemize}
\item $\alpha_i > 0$ = reward for successful attack technique
\item $\beta > 0$ = bonus for compromising valuable assets
\item $\lambda > 0$ = penalty for getting caught
\item $\mathbf{1}[\cdot]$ = indicator function (1 if true, 0 if false)
\end{itemize}
\end{mathdetails}

\subsection{Blue Agent Rewards}

\begin{intuition}
Blue agent gets rewards for:
\begin{itemize}
\item \textbf{Tricking attackers into decoys} (+BIG points)
\item \textbf{Protecting real computers} (+points)
\item \textbf{Using too many resources} (-points) - cost of maintaining decoys
\end{itemize}
\end{intuition}

\begin{mathdetails}
\textbf{Blue Reward Formula}:
$$R^{(b)}_{t,h} = R_{\text{deception}} + R_{\text{protection}} + R_{\text{cost}}$$

Where:
\begin{align}
R_{\text{deception}} &= \begin{cases}
10 \cdot |R_{\text{red}}^{\text{base}}| & \text{if red attacks decoy} \\
0 & \text{otherwise}
\end{cases} \\
R_{\text{protection}} &= \begin{cases}
-|R_{\text{red}}^{\text{base}}| & \text{if red attacks real host} \\
0 & \text{otherwise}
\end{cases} \\
R_{\text{cost}} &= -c_{\text{deploy}} \cdot N_{\text{new decoys}} - c_{\text{maintain}} \cdot \sum_i \text{decoy}_i
\end{align}

\textbf{Key Insight}: The "10×" multiplier for deception means tricking an attacker into a decoy gives 10 times more reward than preventing an attack on a real computer. This strongly encourages the use of deception.
\end{mathdetails}

\section{The PPO Algorithm (How the Agents Actually Learn)}

\begin{foundation}
\textbf{PPO (Proximal Policy Optimization)} is the specific machine learning algorithm we use to train our agents. It's a state-of-the-art method for reinforcement learning.
\end{foundation}

\begin{intuition}
\textbf{Think of PPO like a cautious student}:
\begin{itemize}
\item The student tries new strategies, but not too different from what worked before
\item If a new strategy works well, they adjust their approach slightly in that direction
\item If a new strategy fails, they adjust away from it
\item They never make huge changes all at once (this prevents "forgetting" good strategies)
\end{itemize}
\end{intuition}

\subsection{The PPO Mathematical Formula}

\begin{mathdetails}
\textbf{PPO Objective Function}:
$$L^{\text{PPO}}(\theta) = \mathbb{E}_t\left[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]$$

Let's break this down piece by piece:

\textbf{What is $r_t(\theta)$?}
$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$$

This is the ratio of:
\begin{itemize}
\item New policy probability of taking action $a_t$ in state $s_t$
\item Old policy probability of taking the same action
\end{itemize}

\textbf{What is $\hat{A}_t$?} This is the "advantage" - how much better than average this action was:
\begin{itemize}
\item If $\hat{A}_t > 0$: This action was better than expected
\item If $\hat{A}_t < 0$: This action was worse than expected
\end{itemize}

\textbf{What does "clip" do?} It prevents the ratio $r_t(\theta)$ from getting too big or too small:
\begin{itemize}
\item If $r_t(\theta) < 1-\epsilon$: Set it to $1-\epsilon$ (typically 0.8)
\item If $r_t(\theta) > 1+\epsilon$: Set it to $1+\epsilon$ (typically 1.2)
\item Otherwise: Keep the original value
\end{itemize}

\textbf{Why clip?} This prevents the policy from changing too drastically in one update, which could destabilize learning.
\end{mathdetails}

\section{SULI: Our Novel Training Method}

\begin{foundation}
\textbf{SULI (Self-play with Uniform Learning Initialization)} is our contribution to solving training instability in adversarial RL.
\end{foundation}

\begin{intuition}
\textbf{The Problem with Normal Adversarial Training}:
\begin{itemize}
\item Sometimes one agent learns much faster than the other
\item The fast learner dominates and the slow learner stops improving
\item Training becomes unstable or gets stuck in poor solutions
\end{itemize}

\textbf{SULI Solution}:
\begin{itemize}
\item Start both agents with the same "uniform" strategy (all actions equally likely)
\item Let them learn together gradually
\item Regularly reset if one gets too dominant
\item This creates more balanced, stable learning
\end{itemize}
\end{intuition}

\begin{mathdetails}
\textbf{SULI Mathematical Framework}:

\textbf{Uniform Initialization}:
$$\pi_0^{(b)}(a|s) = \pi_0^{(r)}(a|s) = \frac{1}{|\mathcal{A}|} \quad \forall s \in \mathcal{S}, a \in \mathcal{A}$$

Both agents start with equal probability for all actions.

\textbf{Co-evolution Update}:
\begin{align}
\theta_{k+1}^{(b)} &= \theta_k^{(b)} + \alpha \nabla_\theta L^{\text{PPO}}(\theta_k^{(b)}, \pi_k^{(r)}) \\
\theta_{k+1}^{(r)} &= \theta_k^{(r)} + \alpha \nabla_\theta L^{(r)}(\theta_k^{(r)}, \pi_k^{(b)})
\end{align}

Both agents update simultaneously based on their interaction.

\textbf{Balance Constraint}:
$$|J^{(b)}(\pi_k^{(b)}, \pi_k^{(r)}) - J^{(r)}(\pi_k^{(b)}, \pi_k^{(r)})| \leq \beta$$

If the performance difference gets too large, we intervene to rebalance.
\end{mathdetails}

\section{The Seven-Phase Experimental Methodology}

\begin{foundation}
Our research follows a systematic 7-phase approach to thoroughly validate our methods, from basic functionality to large-scale deployment.
\end{foundation}

\subsection{Phase 1: System Validation}
\begin{intuition}
\textbf{Goal}: Make sure everything works correctly
\begin{itemize}
\item Test with small 15-computer network
\item Verify all software components function
\item Ensure logging and monitoring work
\item Like testing a car before a long road trip
\end{itemize}
\end{intuition}

\subsection{Phase 2: Blue Agent Training}
\begin{intuition}
\textbf{Goal}: Train 8 different defensive strategies
\begin{itemize}
\item Small: Basic defense with limited resources
\item Medium: Balanced detection and deception
\item HighDecoy: Maximum use of decoy computers
\item PerfectDetection: Theoretical best-case scenario
\item (Plus 4 more specialized variants)
\end{itemize}
\end{intuition}

\subsection{Phase 3: Red Agent Training}
\begin{intuition}
\textbf{Goal}: Train 5 different attack strategies
\begin{itemize}
\item RL: Learning-based adaptive attacker
\item ART: Adversarial robustness testing
\item Campaign: Persistent, stealthy attacks
\item (Plus 2 more attack variants)
\end{itemize}
\end{intuition}

\subsection{Phase 4: Cross-Evaluation Matrix}
\begin{intuition}
\textbf{Goal}: Test all combinations of attackers vs defenders
\begin{itemize}
\item 8 defenders × 5 attackers = 40 combinations
\item Like a tournament where every team plays every other team
\item Identifies which defensive strategies work best against which attacks
\end{itemize}
\end{intuition}

\subsection{Phase 5: SULI Co-Evolution}
\begin{intuition}
\textbf{Goal}: Test our novel SULI training method
\begin{itemize}
\item Train both agents simultaneously using SULI
\item Compare against traditional training methods
\item Demonstrate improved stability and performance
\end{itemize}
\end{intuition}

\subsection{Phase 6: Scalability Testing}
\begin{intuition}
\textbf{Goal}: Prove the method works on large networks
\begin{itemize}
\item Test on 1,000, 5,000, and 10,000 computer networks
\item Measure computational requirements and performance
\item Ensure real-world applicability
\end{itemize}
\end{intuition}

\subsection{Phase 7: Statistical Analysis}
\begin{intuition}
\textbf{Goal}: Ensure results are scientifically valid
\begin{itemize}
\item Run multiple experiments with different random seeds
\item Calculate confidence intervals and significance tests
\item Prepare results for academic publication
\end{itemize}
\end{intuition}

\section{Key Evaluation Metrics (How We Measure Success)}

\begin{foundation}
We need concrete ways to measure whether our methods are working. Here are the main metrics we use:
\end{foundation}

\subsection{Deception Effectiveness}
\begin{mathdetails}
$$\text{Deception Rate} = \frac{\text{Number of attacks on decoys}}{\text{Total number of attacks}}$$

\textbf{What this means}: What percentage of attacker actions are wasted on fake computers?
\begin{itemize}
\item 0.0 = Attacker never falls for decoys (bad for defense)
\item 1.0 = Attacker only attacks decoys (perfect defense)
\end{itemize}
\end{mathdetails}

\subsection{Asset Protection Rate}
\begin{mathdetails}
$$\text{Protection Rate} = \frac{\text{Number of uncompromised real computers}}{\text{Total number of real computers}}$$

\textbf{What this means}: What percentage of real computers remain safe?
\begin{itemize}
\item 0.0 = All real computers compromised (complete failure)
\item 1.0 = All real computers protected (perfect success)
\end{itemize}
\end{mathdetails}

\subsection{Mean Time to Compromise (MTTC)}
\begin{mathdetails}
$$\text{MTTC} = \mathbb{E}[\text{Time until first successful attack on critical asset}]$$

\textbf{What this means}: On average, how long does it take an attacker to successfully breach something important?
\begin{itemize}
\item Lower values = Defense fails quickly
\item Higher values = Defense delays attacks successfully
\end{itemize}
\end{mathdetails}

\section{Research Contributions and Impact}

\begin{foundation}
\textbf{What We Discovered}:
\begin{enumerate}
\item SULI training reduces training failures by 90\%
\item Deception-based defense strategies outperform detection-only approaches
\item The framework scales successfully to enterprise-size networks (10,000+ computers)
\item Systematic evaluation reveals optimal defensive strategies for different threat scenarios
\end{enumerate}
\end{foundation}

\begin{intuition}
\textbf{Why This Matters}:
\begin{itemize}
\item \textbf{For Cybersecurity}: Provides concrete guidance on when and how to use deception in network defense
\item \textbf{For AI Research}: Demonstrates how to train stable adversarial agents in complex environments
\item \textbf{For Practice}: Offers scalable methods that could be deployed in real enterprise networks
\item \textbf{For Future Work}: Establishes benchmark methods and metrics for evaluating cybersecurity AI
\end{itemize}
\end{intuition}

\section{Conclusion: The Complete Picture}

\begin{foundation}
This research combines several cutting-edge areas:
\begin{itemize}
\item \textbf{Reinforcement Learning}: Training agents through trial and error
\item \textbf{Adversarial Training}: Training competing agents simultaneously  
\item \textbf{Cybersecurity}: Applying AI to real-world security problems
\item \textbf{Large-Scale Experimentation}: Rigorous scientific validation
\end{itemize}

The result is a comprehensive framework for automatically training cybersecurity defense systems that can adapt to new and evolving threats.
\end{foundation}

\end{document}
