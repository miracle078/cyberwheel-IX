## 5. Environment Formulation

### 5.1 Mathematical Foundation

**Episodic RL Formulation**:
```
Environment: E = (S, A, P, R, Î³, H)
Where:
S = State space (network configurations)
A = Action space (agent decisions)
P = Transition probability function
R = Reward function
Î³ = Discount factor
H = Episode horizon (finite length)
```

**Multi-Agent Extension**:
```
Agents: {Red, Blue} with individual state/action spaces
Red Agent: (S^(r), A^(r), Ï€^(r))
Blue Agent: (S^(b), A^(b), Ï€^(b))
```

**Theoretical Foundation**: This formulation extends standard single-agent MDPs to competitive multi-agent scenarios where agents have opposing objectives, creating a two-player zero-sum game embedded within the RL framework.

### 5.2 Network Representation Theory

**Graph-Theoretic Foundation**:
```
Network Graph: G = (V, E)
Vertices: V = H âˆª S âˆª R
  H = Hosts (computers/devices)
  S = Subnets (network segments)  
  R = Routers (network infrastructure)
Edges: E âŠ† V Ã— V (directed connections)
```

**Implementation Details**:
```python
# NetworkX DiGraph implementation
G = nx.DiGraph()
# Supports 10 to 100,000+ hosts
# YAML-driven configuration system
```

**Host State Vector Mathematical Definition**:
```
h_i = âŸ¨IP_i, OS_i, S_i, V_i, compromised_i, decoy_iâŸ©
Where:
IP_i âˆˆ IPv4_address_space
OS_i âˆˆ {Windows, Linux, macOS, ...}
S_i âŠ† Services (running software)
V_i âŠ† Vulnerabilities (security weaknesses)
compromised_i âˆˆ {0,1}
decoy_i âˆˆ {0,1}
```

**Theoretical Significance**: Each host represents a complex state vector combining network configuration, security posture, and current compromise status, enabling realistic cyber attack/defense simulation.

### 5.3 State Space Mathematics

**Red Agent State Space Formulation**:
```
S^(r) âŠ‚ â„^(d_r)
d_r = 2|H| + |S| + 4 + |T| = 2|H| + |S| + 299

State Vector Components:
S^(r)_{t,h} = [position_{t,h}; knowledge_{t,h}; phase_{t,h}; capabilities_{t,h}]

Where:
position_{t,h} âˆˆ {0,1}^|H|     (one-hot current compromised host)
knowledge_{t,h} âˆˆ {0,1}^(|H|+|S|) (discovered network topology)
phase_{t,h} âˆˆ {0,1}^4          (kill-chain phase encoding)
capabilities_{t,h} âˆˆ {0,1}^|T|  (available MITRE ATT&CK techniques)
```

**Kill-Chain Phase Encoding**:
```
phase âˆˆ {0,1}^4 represents:
[discovery, reconnaissance, privilege-escalation, impact]
Only one phase active at a time (one-hot encoding)
```

**Blue Agent State Space Formulation**:
```
S^(b) âŠ‚ â„^(d_b)  
d_b = 3|H| + 2

State Vector Components:
S^(b)_{t,h} = [alerts_current; alerts_history; decoys; metadata]

Where:
alerts_current âˆˆ {0,1}^|H|    (immediate timestep alerts)
alerts_history âˆˆ {0,1}^|H|    (cumulative alert memory)
decoys âˆˆ {0,1}^|H|            (deployed honeypot locations)
metadata âˆˆ â„^2                (padding constant, total decoy count)
```

**Dual Observation Structure Theory**: The blue agent's state design implements both immediate response capability (current alerts) and strategic learning (historical patterns), enabling both reactive and proactive defensive strategies.

### 5.4 Action Space Formulation

**Red Agent Action Space**:
```
A^(r) = A_discovery âˆª A_recon âˆª A_privesc âˆª A_impact
|A^(r)| = 12 Ã— |H|

Parameterized by target host h âˆˆ H:
A_discovery: Network scanning, ping sweeps, port discovery
A_recon: Service enumeration, vulnerability identification
A_privesc: Exploitation, lateral movement, privilege escalation
A_impact: Data exfiltration, service disruption, persistence
```

**MITRE ATT&CK Integration**:
```
Total Techniques: |T| = 295 verified attack techniques
Kill-chain Mapping: Discovery â†’ Reconnaissance â†’ Privilege Escalation â†’ Impact
Technique Success: Probability-based on target vulnerability profiles
```

**Blue Agent Action Space**:
```
A^(b) = A_deploy âˆª A_remove âˆª A_isolate âˆª {nothing}
|A^(b)| = 2|S||D| + |H| + 1

Action Categories:
A_deploy = {(deploy, s_j, d_k) : s_j âˆˆ S, d_k âˆˆ D}  (deploy decoy type d_k on subnet s_j)
A_remove = {(remove, s_j, d_k) : s_j âˆˆ S, d_k âˆˆ D}  (remove decoys)
A_isolate = {(isolate, h_i) : h_i âˆˆ H}               (isolate compromised hosts)
nothing = no defensive action taken
```

**Strategic Action Design**: The action space reflects realistic cybersecurity operations where defenders must balance resource allocation between detection, deception, and remediation activities.

### 5.5 Reward Function Theory

**Red Agent Reward Structure**:
```
R^(r)_{t,h} = Î£áµ¢ Î±áµ¢Â·ðŸ™[technique_i successful] 
              + Î²Â·|assets_compromised| 
              - Î»Â·ðŸ™[detected]

Parameter Interpretation:
Î±áµ¢ > 0: Technique execution rewards (encourages attack progression)
Î² > 0: Asset compromise value (objective achievement)
Î» > 0: Detection penalty (stealth incentive)
```

**Blue Agent Reward Structure**:
```
R^(b)_{t,h} = R_deception + R_protection + R_cost

Component Breakdown:
R_deception = 10Â·|R^(r)_base| if red attacks decoy successfully
R_protection = -|R^(r)_base| if red attacks real host successfully  
R_cost = -c_deployÂ·N_new_decoys - c_maintainÂ·Î£áµ¢ decoy_i
```

**Critical Design Decision - 10Ã— Deception Multiplier**:
```
Theoretical Justification:
1. Creates strong incentive alignment toward honeypot effectiveness
2. Reflects real-world value of attack misdirection
3. Encourages sophisticated defensive strategies
4. Verified in implementation (rl_reward.py, lines 142-143)
```

**Adversarial Reward Structure**: The reward functions are designed to be zero-sum in the core attack/defense interaction while allowing for operational costs and strategic considerations.

### 5.6 State Transition Dynamics

**Transition Probability Formulation**:
```
P(S_{t,h+1}, S^(r)_{t,h+1}, S^(b)_{t,h+1} | S_{t,h}, S^(r)_{t,h}, S^(b)_{t,h}, A^(r)_{t,h}, A^(b)_{t,h})
```

**Decomposition**:
```
Network State Transition: P(N_{t,h+1} | N_{t,h}, A^(r)_{t,h}, A^(b)_{t,h})
Red State Update: P(S^(r)_{t,h+1} | N_{t,h+1}, S^(r)_{t,h}, A^(r)_{t,h})
Blue State Update: P(S^(b)_{t,h+1} | N_{t,h+1}, S^(b)_{t,h}, A^(r)_{t,h}, A^(b)_{t,h})
```

**Deterministic Network Rules**:
- Red actions modify host compromise status based on vulnerability exploitation
- Blue actions add/remove decoys and modify network isolation policies
- Alert generation follows probabilistic detection models

**Probabilistic Alert Generation**:
```
P(alert | technique, detector) = Î _j p_j^(detector)
Where j indexes technique components
```

---

## 6. Algorithmic Framework

### 6.1 Historical Context Formulation

**Complete History Definition**:
```
H_{t,h} = {(S^(r)_{t',h'}, A^(r)_{t',h'}, S^(b)_{t',h'}, A^(b)_{t',h'}, 
           R^(r)_{t',h'}, R^(b)_{t',h'})}_{(t',h') < (t,h)}
```

**Theoretical Significance**: This comprehensive history enables sophisticated strategy adaptation and opponent modeling, crucial for adversarial learning scenarios.

**Memory Complexity**: History grows linearly with episode length, requiring efficient storage and retrieval mechanisms for practical implementation.

### 6.2 Red Agent Algorithm Analysis

**Deterministic Baseline Algorithm**:
```
Algorithm: Deterministic Kill-Chain Policy
Input: State S^(r)_{t,h}, network knowledge
1. Extract phase Ï† and position p from state
2. Phase-based action selection:
   if Ï† = discovery: network_scan(current_subnet)
   elif Ï† = reconnaissance: vulnerability_identification(discovered_hosts)
   elif Ï† = privilege_escalation: lateral_movement(target_selection)
   elif Ï† = impact: data_exfiltration(high_value_assets)
3. Phase transition based on success criteria
4. Return action A^(r)_{t,h}
```

**Phase Transition Logic**:
```
discovery â†’ reconnaissance: sufficient topology discovered
reconnaissance â†’ privilege_escalation: exploitable vulnerability found
privilege_escalation â†’ impact: critical server compromised
impact â†’ [episode termination or persistence]
```

**Adaptive Enhancement Framework**:
```
Ï€^(r)(a|s,H) = softmax(Î²Â·Q^(r)(s,a) + Î±Â·adaptation_bonus(a,H))

adaptation_bonus rewards actions that:
1. Counter observed blue defensive patterns
2. Exploit detected blue agent weaknesses
3. Adapt to blue agent behavioral changes
```

### 6.3 PPO Algorithm Deep Dive

**Proximal Policy Optimization Core Objective**:
```
L^PPO(Î¸) = ð”¼_t[min(r_t(Î¸)Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã‚_t)]

Where:
r_t(Î¸) = Ï€_Î¸(A^(b)_{t,h}|S^(b)_{t,h}) / Ï€_{Î¸_old}(A^(b)_{t,h}|S^(b)_{t,h})  (probability ratio)
Ã‚_t = Generalized Advantage Estimate
Îµ = 0.2 (clipping parameter, verified in implementation)
```

**Theoretical Foundation of Clipping**:
- **Problem**: Large policy updates can destabilize training
- **Solution**: Clip probability ratios to [1-Îµ, 1+Îµ] range
- **Guarantee**: Monotonic policy improvement with bounded updates

**Generalized Advantage Estimation (GAE)**:
```
Ã‚_{t,h} = Î£Ë¡â¼â°^{H-h} (Î³Î»)Ë¡ Î´_{t,h+l}

Where:
Î´_{t,h} = R^(b)_{t,h} + Î³V(S^(b)_{t,h+1}) - V(S^(b)_{t,h})  (TD error)
Î» = 0.95 (GAE parameter, bias-variance tradeoff)
Î³ = 0.95 (discount factor)
```

**GAE Theory**:
- **Î» = 0**: High bias, low variance (TD(0))
- **Î» = 1**: Low bias, high variance (Monte Carlo)
- **Î» = 0.95**: Optimal bias-variance balance for most applications

### 6.4 PPO Training Algorithm Implementation

**Three-Phase Training Cycle**:

**Phase 1: Experience Collection**
```
Algorithm: Experience Collection
for episode t = 1 to T:
    for timestep h = 1 to H:
        observe state S^(b)_{t,h}
        sample action A^(b)_{t,h} ~ Ï€_Î¸(S^(b)_{t,h})
        execute action in environment
        observe reward R^(b)_{t,h} and next state S^(b)_{t,h+1}
        store transition (S^(b)_{t,h}, A^(b)_{t,h}, R^(b)_{t,h}, S^(b)_{t,h+1})
```

**Phase 2: Advantage Computation**
```
Algorithm: GAE Computation
compute value estimates V_Î¸(S^(b)_{t,h}) for all states
compute TD errors Î´_{t,h} = R^(b)_{t,h} + Î³V_Î¸(S^(b)_{t,h+1}) - V_Î¸(S^(b)_{t,h})
compute advantages Ã‚_{t,h} using GAE formula
compute returns R^total_{t,h} = Ã‚_{t,h} + V_Î¸(S^(b)_{t,h})
normalize advantages: Ã‚_{t,h} â† (Ã‚_{t,h} - mean(Ã‚)) / std(Ã‚)
```

**Phase 3: Policy Update**
```
Algorithm: PPO Policy Update
for epoch k = 1 to K:
    for each minibatch in experience buffer:
        compute probability ratios r_t(Î¸)
        compute clipped objective L^PPO(Î¸)
        compute value loss L^value = Â½(V_Î¸(s) - R^total)Â²
        compute entropy bonus L^entropy = -Î²_ent Î£_a Ï€_Î¸(a|s)log Ï€_Î¸(a|s)
        total loss L^total = L^PPO + 0.5Â·L^value + 0.01Â·L^entropy
        gradient update Î¸ â† Î¸ - Î±âˆ‡_Î¸ L^total
```

**Loss Function Components**:
1. **PPO Loss**: Primary policy improvement objective
2. **Value Loss**: Critic network training (coefficient: 0.5)
3. **Entropy Bonus**: Exploration encouragement (coefficient: 0.01)

### 6.5 Detection and Alert Mechanisms

**Alert Structure Definition**:
```
Alert_{t,h} = âŸ¨src_host, techniques, timestamp, confidenceâŸ©

Where:
src_host âˆˆ H âˆª {null}          (source of suspicious activity)
techniques âŠ† T                 (detected MITRE ATT&CK techniques)
timestamp âˆˆ â„•                  (timestep of detection)
confidence âˆˆ [0,1]             (detection confidence score)
```

**Detection Probability Model**:
```
Multi-Component Technique Detection:
p_detect(technique_i, detector_d) = Î _{j=1}^{|technique_i.components|} p_j^(d)

Multi-Technique Detection (at least one detected):
P(detect â‰¥ 1) = 1 - Î _i (1 - p_detect(technique_i, detector_d))
```

**Detection Model Calibration**:
- **Simple Techniques**: p_detect â‰ˆ 0.9 (high detection probability)
- **Sophisticated Techniques**: p_detect â‰ˆ 0.3 (moderate detection)
- **Stealth Techniques**: p_detect â‰ˆ 0.1 (low detection probability)

**False Positive Generation Model**:
```
P(false positive at time t) = 1 - e^{-Î»_fpÂ·Î”t}

Where:
Î»_fp = false positive rate parameter
Î”t = time interval
```

**Exponential Distribution Rationale**: False positives often result from random system events, network noise, and operational activities, which follow Poisson processes leading to exponential inter-arrival times.

### 6.6 Blue Agent Observation Vector Construction

**Verified Implementation Algorithm**:
```
Algorithm: Blue Observation Construction
Input: Current alerts A_t, host mapping, active decoys
Output: Observation vector o_t âˆˆ â„^{d_b}

1. Initialize o_t â† 0^{d_b}
2. barrier â† |H|
3. Reset current alerts: o_t[0:barrier-1] â† 0
4. for each alert a âˆˆ A_t:
     if a.src_host â‰  null and mapping exists:
         o_t[mapping[a.src_host]] â† 1              (current alert)
         o_t[barrier + mapping[a.src_host]] â† 1    (sticky history)
5. Set metadata:
   o_t[d_b-2] â† -1                                (padding constant)
   o_t[d_b-1] â† |active_decoys|                   (decoy count)
6. return o_t
```

**Sticky Memory Design**: Once an alert is generated for a host, the historical alert bit remains set, providing the blue agent with persistent memory of past suspicious activity.

**Implementation Verification**: This algorithm is verified against the actual codebase implementation, ensuring accuracy of the mathematical description.

---

*This completes the technical analysis of sections 5-6. Would you like me to continue with sections 7-10 covering Evaluation Methodology, Experimental Design, Results Analysis, and Mathematical Foundations?*
